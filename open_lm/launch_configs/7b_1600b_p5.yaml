accum-freq: 1
beta1: 0.9
beta2: 0.95
data-key: "json"
dataset-resampled: True
delete-previous-checkpoint: False
# Total 5B * 320 = 1.6T tokens
epochs: 320
fsdp: True
fsdp-limit-all-gathers: True
grad-checkpointing: True
grad-clip-norm: 1
log-every-n-steps: 20
model: "open_lm_7b"
name: "7b_p5"
precision: "amp_bfloat16"
report-to: "wandb"
seed: 124
train-data-mix-weights: "0.725::0.275"
train-data: "openlm_mix_tri_s3"
train-num-samples: 5_000_000_000
wandb-project-name: "lm1"
workers: 4

# Some important parameters, double checked with Mitchell:
batch-size: 16
ffn-type: swiglu
fsdp-amp: True
fsdp-pure-bf16: False
lr: 1e-3
lr-cooldown-end: 3e-5
model-norm: "gain_only_layer_norm"
qk-norm: True
warmup: 5000
wd: 0.1
z-loss-coefficient: 1e-4
resume: s3://tri-ml-sandbox-16011-us-east-1-datasets/sagemaker/achal.dave/openlm-p5/achal-dave-openlm-p5_7b_p5_2023-10-24-14-18-09-185/checkpoint/7b_p5/checkpoints/epoch_135.pt
# resume: ./checkpoints/s3/tri-ml-sandbox-16011-us-east-1-datasets/sagemaker/achal.dave/openlm-p5/achal-dave-openlm-p5_7b_p5_2023-10-24-14-18-09-185/checkpoint/7b_p5/checkpoints/epoch_135.pt
